{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to install the required packages\n",
    "# %pip install pandas sqlalchemy mysqlclient\n",
    "# %pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# database connection\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "# data visualization and plots creation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Read CSV and calculate daily percentage change for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate initial DataFrame from CSV file\n",
    "def load_and_prepare_data(csv_path):\n",
    "    \"\"\"\n",
    "    Load data from CSV, clean, and prepare for analysis.\n",
    "\n",
    "    Parameters:\n",
    "    csv_path (str): The path to the CSV file containing the stock data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the daily percentage change of the closing price for each stock.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Remove any duplicates based on 'Date' and 'Symbol' before pivoting\n",
    "        df.drop_duplicates(subset=['Date', 'Symbol'], inplace=True)\n",
    "        # Ensure no NaN values in 'Close' before calculating 'Percent Change'\n",
    "        df.dropna(subset=['Close'], inplace=True)\n",
    "        # Sort by 'Date' and 'Symbol' to ensure chronological order for percent change calculation\n",
    "        df.sort_values(by=['Date', 'Symbol'], inplace=True, ascending=[True, True])\n",
    "        # Calculate the daily percentage change of the closing price for each stock\n",
    "        df['Percent Change'] = df.groupby('Symbol')['Close'].pct_change() * 100\n",
    "        print(\"Dataframe loaded and prepared for analysis\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataframe from csv file {e}\")\n",
    "        \n",
    "## example usage\n",
    "# csv_path = 'stocks.csv'\n",
    "# df = load_and_prepare_data(csv_path)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Reports generation for stocks analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate Biggest Move report\n",
    "def generate_reports(df):\n",
    "    \"\"\"\n",
    "    Generate reports and visualizations from the stock data.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The cleaned dataframe from csv file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Two DataFrames containing the Biggest Move Report and Average Percent Change Report\n",
    "    \"\"\"\n",
    "    try: \n",
    "        # Biggest Move Report\n",
    "        # Identify the stock with the biggest absolute percentage change for each day\n",
    "        df['Abs Percent Change'] = df['Percent Change'].abs()\n",
    "        # Get the index of max absolute percentage change for each day\n",
    "        idx = df.groupby('Date')['Abs Percent Change'].idxmax()\n",
    "        # Remove NaN values from idx to ensure it only contains valid indices\n",
    "        idx = idx.dropna()\n",
    "        # Use the index to locate the rows in df\n",
    "        biggest_move_report = df.loc[idx, ['Date', 'Symbol', 'Abs Percent Change']]\n",
    "\n",
    "        # Average Percent Change Report\n",
    "        avg_percent_change_per_stock = df.groupby('Symbol')['Percent Change'].mean().reset_index()\n",
    "        avg_percent_change_per_stock.columns = ['Symbol', 'Avg Percent Change']\n",
    "        \n",
    "        return biggest_move_report, avg_percent_change_per_stock\n",
    "    \n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError occurred: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        # Optionally, return empty DataFrames or None in case of error\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "## example usage\n",
    "# csv_path = 'stocks.csv'\n",
    "# df = load_and_prepare_data(csv_path)\n",
    "# print(generate_reports(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Save Reports to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_to_database(df_list, table_names, database_credentials):\n",
    "#     \"\"\"\n",
    "#     Saves a list of DataFrames to a MySQL database using provided credentials and table names.\n",
    "\n",
    "#     Args:\n",
    "#         df_list (list of pandas.DataFrame): List of DataFrames to save.\n",
    "#         table_names (list of str): Corresponding list of table names for the DataFrames.\n",
    "#         database_credentials (dict): Dictionary with database connection details.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Construct the MySQL connection string\n",
    "#         database_url = f\"mysql+mysqlconnector://{database_credentials['user']}:\" \\\n",
    "#                         f\"{database_credentials['password']}@{database_credentials['host']}:\" \\\n",
    "#                         f\"{database_credentials['port']}/{database_credentials['dbname']}\"\n",
    "        \n",
    "#         # Create database engine\n",
    "#         engine = create_engine(database_url)\n",
    "\n",
    "#         # Save each DataFrame to its respective table\n",
    "#         for df, table_name in zip(df_list, table_names):\n",
    "#             df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        \n",
    "#         print(\"DataFrames saved to MySQL database successfully.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving DataFrames to database: {e}\")\n",
    "\n",
    "# Example usage\n",
    "# database_credentials = {\n",
    "#     'user': 'root',\n",
    "#     'password': 'password',\n",
    "#     'host': 'localhost',\n",
    "#     'port': '3306',\n",
    "#     'dbname': 'stock_analysis'\n",
    "# }\n",
    "\n",
    "# # Assume 'load_and_prepare_data' and 'generate_reports' are defined and return the necessary DataFrames\n",
    "# csv_path = 'stocks.csv'\n",
    "# df = load_and_prepare_data(csv_path)\n",
    "# biggest_move_report, avg_percent_change_per_stock = generate_reports(df)\n",
    "# df_list = [biggest_move_report, avg_percent_change_per_stock]\n",
    "# table_names = ['biggest_move_report', 'avg_percent_change_per_stock']\n",
    "\n",
    "# # Call the optimized save_to_database function\n",
    "# save_to_database(df_list, table_names, database_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Save Reports to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_database(df_list, table_names, db_path):\n",
    "    \"\"\"\n",
    "    Saves a list of DataFrames to an SQLite database using provided file path.\n",
    "\n",
    "    Args:\n",
    "        df_list (list of pandas.DataFrame): List of DataFrames to save.\n",
    "        table_names (list of str): Corresponding list of table names for the DataFrames.\n",
    "        db_path (str): Path to the SQLite database file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the SQLite connection string\n",
    "        database_url = f\"sqlite:///{db_path}\"\n",
    "        \n",
    "        # Create database engine\n",
    "        engine = create_engine(database_url)\n",
    "\n",
    "        # Save each DataFrame to its respective table\n",
    "        for df, table_name in zip(df_list, table_names):\n",
    "            df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        \n",
    "        print(\"DataFrames saved to SQLite database successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrames to database: {e}\")\n",
    "\n",
    "## Example usage\n",
    "# csv_path = 'stocks.csv'  # CSV file relative path\n",
    "# db_path = 'stock_analysis.db'  # SQLite database relative file path\n",
    "# df = load_and_prepare_data(csv_path)\n",
    "# biggest_move_report, avg_percent_change_per_stock = generate_reports(df)\n",
    "# df_list = [biggest_move_report, avg_percent_change_per_stock]\n",
    "# table_names = ['biggest_move_report', 'avg_percent_change_per_stock']\n",
    "\n",
    "# # Call the revised save_to_database function for SQLite\n",
    "# save_to_database(df_list, table_names, db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. SQL script to get the number of top performing days for each stock:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT \n",
    "\ta.Symbol AS `Symbol`,\n",
    "\tIFNULL(b.`Top Performing Days`, 0) AS `Top Performing Days`\n",
    "FROM \n",
    "\t(SELECT Symbol FROM avg_percent_change_per_stock) a\n",
    "LEFT JOIN \n",
    "\t(SELECT Symbol, COUNT(*) AS `Top Performing Days` FROM biggest_move_report GROUP BY Symbol) b\n",
    "ON a.Symbol = b.Symbol\n",
    "ORDER BY `Top Performing Days` DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI. Create Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_stock_correlation(df, output_path):\n",
    "    \"\"\"\n",
    "    Visualizes the correlation matrix of daily returns for each stock.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing stock data.\n",
    "    output_path (str): The path to save the output image file.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Pivot the DataFrame to get daily returns for each stock\n",
    "    daily_returns = df.pivot(index='Date', columns='Symbol', values='Percent Change')\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = daily_returns.corr()\n",
    "    # Visualizing the correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "    plt.title('Stock Correlation Matrix')\n",
    "    # (Optional) Save the figure as an image file\n",
    "    plt.savefig(output_path)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# # example usage\n",
    "# csv_path = 'stocks.csv'\n",
    "# df = load_and_prepare_data(csv_path)\n",
    "# visualize_stock_correlation(df,'correlation_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Lack of entries for GOOGL.A, GOOGL.z and RDDT in the stocks.csv dataset caused NaN Daily Percentage Change, hence there's no correlation between these stocks with others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded and prepared for analysis\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'db_details' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m load_and_prepare_data(csv_path)\n\u001b[0;32m      5\u001b[0m biggest_move_report, avg_percent_change \u001b[38;5;241m=\u001b[39m generate_reports(df)\n\u001b[1;32m----> 6\u001b[0m save_to_database([biggest_move_report, avg_percent_change], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiggest_move_report\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_percent_change_per_stock\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mdb_details\u001b[49m)\n\u001b[0;32m      7\u001b[0m visualize_correlation(df, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrelation_matrix.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'db_details' is not defined"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = 'stocks.csv'\n",
    "    df = load_and_prepare_data(csv_path)\n",
    "    biggest_move_report, avg_percent_change = generate_reports(df)\n",
    "    df_list = [biggest_move_report, avg_percent_change]\n",
    "    table_names = ['biggest_move_report', 'avg_percent_change_per_stock']\n",
    "    db_path = 'stock_analysis.db'\n",
    "    save_to_database(df_list, table_names, db_path)\n",
    "    visualize_correlation(df, output_path='correlation_matrix.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
